<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hyelin Nam</title>

    <meta name="author" content="Hyelin Nam">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12345678-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-KMX6XX574P');
    </script>	  
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hyelin Nam
                </p>
                <p>
                  I'm a 1st year PhD Student at <a href="https://cse.engin.umich.edu/">Umich CSE</a>, advised by Prof. <a href="https://jjparkcv.github.io/">JJ Park</a>. 
                  Before that, I completed my master's at <a href="https://gsai.kaist.ac.kr/">KAIST</a>, 
                  where I was advised by Prof. <a href="https://bispl.weebly.com/">Jong Chul Ye</a>.
                </p> 
                
                <p>
                  Recently, I have worked on <b>diffusion models</b>, leveraging their adaptability to enhance the quality of generated outputs.  
                  My goal is to advance <b>generative models to accurately capture real-world dynamics</b>, ultimately enabling them to function as reliable real-world priors.
                </p>

          <p style="text-align:center">
            <a href="mailto:hyelinam@umich.edu">Email</a> &nbsp;/&nbsp;
            <a href="https://drive.google.com/file/d/1ZA-_nTiaNIV2YXGFzzngk6fc3uN5M9mx/view?usp=sharing">CV</a> &nbsp;/&nbsp;
            <a href="https://scholar.google.com/citations?user=k6nzq08AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
            <a href="https://github.com/HyelinNAM">Github</a> &nbsp;&nbsp;
            <!-- <a href="https://comlini8-8.tistory.com/">Blog (Korean)</a> -->
          </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/hyelin.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/hyelin.jpg" class="hoverZoomLink"></a>
            </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
    <tr style="background-color: #ffffff;">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mp_gif'>
                  <img src='images/videorsplat.png' width="180" style="margin-top: 30px">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle;">
              [C6]
                <a href="https://gohyojun15.github.io/VideoRFSplat/">
              <span class="papertitle">
                VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling
              </span>
                </a>
                <br>
                Hyojun Go*,
                Byeongjun Park*, 
                <strong>Hyelin Nam</strong>,
                Byung-Hoon Kim,
                Hyungjin Chung,
                Changick Kim
                <br>
                ICCV, 2025
                <br>
                <a href="https://gohyojun15.github.io/VideoRFSplat/">project page</a>
                /
                <a href="https://arxiv.org/abs/2503.15855">arXiv</a>
                / 
                <a href="https://github.com/gohyojun15/VideoRFSplat">code</a>
              
                <p>
                  A text-to-3D method using a video generation model to jointly generate diverse camera poses and realistic 3DGS for unbounded scenes.
                </p>
              </td>
    </tr>

    <tr style="background-color: #ffffff;">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mp_gif'>
                  <img src='images/steerx.jpeg' width="180" style="margin-top: 30px">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle;">
              [C5]
                <a href="https://byeongjun-park.github.io/SteerX/">
              <span class="papertitle">
                SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering
              </span>
                </a>
                <br>
                Byeongjun Park*, 
                Hyojun Go*,
                <strong>Hyelin Nam</strong>,
                Byung-Hoon Kim,
                Hyungjin Chung,
                Changick Kim
                <br>
                ICCV, 2025
                <br>
                <em style="color: rgb(190, 56, 208);">CVPR 2025 Workshop on WorldModelBench</em>
                <br>
                <a href="https://motionprompt.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2503.12024">arXiv</a>
                / 
                <a href="hhttps://github.com/byeongjun-park/SteerX">code</a>
              
                <p>
                  A zero-shot inference-time steering method that enhances geometric alignment in 3D/4D scene generation by integrating scene reconstruction using pose-free geometric reward functions.
                </p>
              </td>
    </tr>

    <tr style="background-color: #ffffff;">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='mp_gif'>
              <img src='images/mp_teaser.gif' width="160" style="margin-left: 8px">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle;">
          [C4]
            <a href="https://github.com/HyelinNAM/MotionPrompt">
          <span class="papertitle">
            Optical-Flow Guided Prompt Optimization for Coherent Video Generation
          </span>
            </a>
            <br>
            <strong>Hyelin Nam</strong>*,
            Jaemin Kim*, 
            Dohun Lee,
            Jong Chul Ye
            <br>
            CVPR, 2025
            <br>
            <a href="https://motionprompt.github.io/">project page</a>
            /
            <a href="https://arxiv.org/pdf/2411.15540">arXiv</a>
            / 
            <a href="https://github.com/HyelinNAM/MotionPrompt">code</a>
          
            <p>
              Prompt optimization driven by an optical flow discriminator to enhance temporal consistency and natural motion dynamics in video diffusion models.
            </p>
          </td>
    </tr>

    <tr style="background-color: #ffffff;">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cfgpp_img'>
          <img src='images/cfg++_main.png' width="180">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        [C3]
        <a href="https://cfgpp-diffusion.github.io/">
			<span class="papertitle">
        CFG++: Manifold-constrained Classifier Free Guidance For Diffusion Models
</span>
        </a>
        <br>
				Hyungjin Chung*,
        Jeongsol Kim*, 
        Geon Yeong Park*,
        <strong>Hyelin Nam</strong>*,
				Jong Chul Ye
        <br>
        ICLR, 2025
        <br>
        <em style="color: rgb(208, 66, 56);">Silver prize, 31st Samsung Humantech Paper Award</em>
        <br>
        <a href="https://cfgpp-diffusion.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2406.08070">arXiv</a>
        /
        <a href="https://github.com/CFGpp-diffusion/CFGpp">code</a>
        <p></p>
        <p>
          A simple fix to CFG that enables lower guidance scales, improves sample quality and invertibility.
        </p>
      </td>
    </tr>

    <tr style="background-color: #ffffff;">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cds_main'>
          <img src='images/cds_main.png' width="180">
        </div>
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        [C2]
        <a href="https://hyelinnam.github.io/CDS">
          <span class="papertitle"> Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing
</span>
        </a>
        <br>
        <strong>Hyelin Nam</strong>,
        Gihyun Kwon,
        Geon Yeong Park,
				Jong Chul Ye
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="https://hyelinnam.github.io/CDS/">project page</a>
        /
        <a href="https://arxiv.org/abs/2311.18608">arXiv</a>
        /
        <a href="https://github.com/HyelinNAM/ContrastiveDenoisingScore">code</a>
        <p></p>
        <p>
          Ensure structural correspondence by leveraging diffusion features during the score distillation process.
      </td>
    </tr>

    <tr style="background-color: #ffffff;">
      <td style="padding:25px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='hairfit_main'>
          <img src='images/hairfit_main.png' width="180" style="margin-top: 13px">
        </div>
      </td>
      
      <td style="padding:20px;width:75%;vertical-align:middle">
      [C1]
			<span class="papertitle">HairFIT: Pose-invariant Hairstyle Transfer via Flow-based Hair Alignment and Semantic-region-aware Inpainting
</span>
        </a>
        <br>
        Chaeyeon Chung*,
        Taewoo Kim*,
        <strong>Hyelin Nam</strong>*,
        Seunghwan Choi,
        Gyojung Gu,
        Sunghyun Park,
        Jaegul Choo
        <br>
        <em>BMVC</em>, Oral Presentation, 2022 <br>
        <em style="color: rgb(208, 66, 56);">Best Paper Award, Korean Artificial Intelligence Association, 2021</em>
        <br>
        <a href="https://arxiv.org/abs/2206.08585">arXiv</a>
        <p></p>
      </td>
    </tr>

    

          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
